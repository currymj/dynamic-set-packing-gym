{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_dynamic_set_packing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is the code for the training loop. after that are definitions of two simple agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(env, agent, episode_count, batches_per_episode, max_steps, quiet=False):\n",
    "    done = False\n",
    "    ep_rewards = []\n",
    "    for i in tqdm(range(episode_count)):\n",
    "        reward = 0.0\n",
    "        if not quiet:\n",
    "            print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for batch_idx in range(batches_per_episode):\n",
    "            curr_obs = []\n",
    "            curr_act = []\n",
    "            curr_reward = []\n",
    "            for i in range(max_steps):\n",
    "                curr_obs.append(ob)\n",
    "                action = agent.act(ob, reward, done)\n",
    "                curr_act.append(action)\n",
    "                ob, reward, done, _ = env.step(action)\n",
    "                curr_reward.append(reward)\n",
    "                if not quiet:\n",
    "                    print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "            history_dict['observations'].append(np.stack(curr_obs,axis=0))\n",
    "            history_dict['actions'].append(curr_act)\n",
    "            history_dict['rewards'].append(curr_reward)\n",
    "        total_reward = np.sum(history_dict['rewards']) / batches_per_episode\n",
    "        agent.learn(history_dict)\n",
    "        history_dict = None\n",
    "        if not quiet:\n",
    "            print('total episode reward: {}'.format(total_reward))\n",
    "        ep_rewards.append(total_reward)\n",
    "    return ep_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatchAgent:\n",
    "    \"A simple agent for the 0/1 problem that always matches.\"\n",
    "    def __init__(self, match_prob):\n",
    "        self.policy_dist = torch.distributions.Categorical(torch.tensor([match_prob, 1 - match_prob], dtype=torch.float32))\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        action_sample = self.policy_dist.sample()\n",
    "        return action_sample.item()\n",
    "    \n",
    "    def learn(self, history_dict):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_episode_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Given a sequence of rewards, returns the sequence\n",
    "    of the discounted returns (G_t) at each time step,\n",
    "    with discount rate gamma (default 0.999).\n",
    "    \"\"\"\n",
    "    # thanks to yuhao for writing this code for another project\n",
    "    length = len(rewards)\n",
    "    discounts = [gamma**x for x in range(length)]\n",
    "    result = [np.dot(discounts[:length-i], rewards[i:]) for i in range(length)]\n",
    "    return np.array(result, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_target(policy_dist, rewards, action_trajectory):\n",
    "    \"\"\"\n",
    "    The policy gradient target loss (without baseline). Note it should be negative because\n",
    "    optimizers minimize by default. Rewards should be cumulative and discounted.\n",
    "    All inputs should already be tensors, not lists or np arrays.\n",
    "    \"\"\"\n",
    "    return -torch.mean(policy_dist.log_prob(action_trajectory)*rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_baseline_target(policy_dist, delta, action_trajectory):\n",
    "    \"\"\"\n",
    "    The policy gradient target loss (with baseline). Note it should be negative because\n",
    "    optimizers minimize by default. delta should be discounted rewards minus value estimates. \n",
    "    All inputs should already be tensors, not lists or np arrays.\n",
    "    \"\"\"\n",
    "    return -torch.mean(policy_dist.log_prob(action_trajectory)*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMatchAgent:\n",
    "    def __init__(self, observation_shape, gamma=0.99, gpu=False):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(observation_shape, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(observation_shape, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "\n",
    "        self.gpu = gpu\n",
    "        if gpu:\n",
    "            self.policy_net.cuda()\n",
    "            self.value_net.cuda()\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy_net.parameters()) + list(self.value_net.parameters()))\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def policy(self, observation_batch):\n",
    "        return torch.distributions.Categorical(logits=self.policy_net(observation_batch))\n",
    "    \n",
    "    def value(self, observation_batch):\n",
    "        return self.value_net(observation_batch)\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        \"Act on a single observation, return an action.\"\n",
    "        observation_as_batch = torch.tensor(np.expand_dims(observation, 0), dtype=torch.float32, requires_grad=False, device=self.device)\n",
    "        action_sample = self.policy(observation_as_batch).sample().detach().cpu().numpy()\n",
    "        return action_sample[0]\n",
    "    \n",
    "    def learn(self, history_dict):\n",
    "        \"\"\"\n",
    "        Perform the policy gradient update with its optimizer and policy.\n",
    "        \n",
    "        history_dict in general contains batches of episodes; we flatten these out\n",
    "        into one enormous batch (as long as discounting of rewards is done correctly, the policy gradient loss\n",
    "        doesn't care where each example came from).\n",
    "        \"\"\"\n",
    "        \n",
    "        # discounted returns, flattened out\n",
    "        disc_returns = torch.as_tensor(np.concatenate([discounted_episode_returns(r, gamma=self.gamma) for r in history_dict['rewards']]), dtype=torch.float32, device=self.device)\n",
    "        observations_tensor = torch.as_tensor(np.vstack(history_dict['observations']), dtype=torch.float32, device=self.device)\n",
    "        actions_tensor = torch.as_tensor(np.concatenate(history_dict['actions']), dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_dists = self.policy(observations_tensor)\n",
    "        value_estimates = self.value(observations_tensor)\n",
    "        delta = disc_returns - value_estimates\n",
    "        value_loss = delta.mean() # delta.pow(2) instead?\n",
    "        \n",
    "        policy_loss = pg_target(policy_dists,\n",
    "                             delta,\n",
    "                             actions_tensor)\n",
    "        torch.autograd.backward([value_loss, policy_loss])\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history dict consists of:\n",
    "\n",
    "B x obs_dim x episode_length\n",
    "\n",
    "returns consists of\n",
    "\n",
    "B x episode length\n",
    "\n",
    "actions consists of\n",
    "\n",
    "B x episode length\n",
    "\n",
    "we want to apply discounted_episode_returns to each 1 x episode_length return sequence\n",
    "\n",
    "then it should be safe to flatten and concatenate all these, and make one update with the pg loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blood types example (fast, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_example = gym.make('DynamicSetPacking-gurobitest-v0')\n",
    "ag = MLPMatchAgent(env_example.observation_space.shape[0], gamma=0.999)\n",
    "ep_rewards = train_loop(env_example, ag, 100, 4, 10, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blood types example (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_example = gym.make('DynamicSetPacking-gurobitest-v0')\n",
    "ag = MLPMatchAgent(env_example.observation_space.shape[0], gamma=0.999, gpu=True)\n",
    "ep_rewards = train_loop(env_example, ag, 100, 32, 50, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare to random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_example = gym.make('DynamicSetPacking-gurobitest-v0')\n",
    "ag = RandomMatchAgent(0.5)\n",
    "random_ep_rewards = train_loop(env_example, ag, 1000, 50, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(random_ep_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_rewards, label='reinforce')\n",
    "plt.plot(random_ep_rewards, label='random')\n",
    "plt.title('returns per 50-step episode')\n",
    "plt.legend()\n",
    "plt.savefig('returns_plot.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "e268f7207fa44ead9c2bffd263bb3988",
   "lastKernelId": "de0efd1a-d4a9-4f98-b7cb-ce6616f8f9dd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
