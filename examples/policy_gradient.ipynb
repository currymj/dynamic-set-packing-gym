{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_dynamic_set_packing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(tf, 'enable_eager_execution'):\n",
    "    tf.enable_eager_execution()\n",
    "# we do want eager execution, because that's the norm for TF2.0\n",
    "# don't forget to test occasionally to see how it works in the alpha, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_episode_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Given a sequence of rewards, returns the sequence\n",
    "    of the discounted returns (G_t) at each time step,\n",
    "    with discount rate gamma (default 0.999).\n",
    "    \"\"\"\n",
    "    # thanks to yuhao for writing this code for another project\n",
    "    length = len(rewards)\n",
    "    discounts = [gamma**x for x in range(length)]\n",
    "    result = [np.dot(discounts[:length-i], rewards[i:]) for i in range(length)]\n",
    "    return np.array(result, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_target(policy_dist, rewards, action_trajectory):\n",
    "    \"\"\"\n",
    "    The policy gradient target loss (without baseline). Note it has to be negative,\n",
    "    because TF optimizers only want to minimize. Rewards should be cumulative and discounted.\n",
    "    All inputs should already be TensorFlow objects, not lists or np arrays.\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(policy_dist.log_prob(action_trajectory)*rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMatchAgent:\n",
    "    def __init__(self, observation_shape, gamma=0.99):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        # policy network uses TFP layer at the end.\n",
    "        self.policy = tf.keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=observation_shape),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2),\n",
    "            tfp.layers.OneHotCategorical(2)\n",
    "        ])\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "        self.gamma = gamma\n",
    "        ## need to compile for non-eager mode?\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        \"Act on a single observation, return an action.\"\n",
    "        observation_as_batch = tf.constant(np.expand_dims(observation, 0), dtype=tf.float32)\n",
    "        action_sample = tf.stop_gradient(self.policy(observation_as_batch).sample())\n",
    "        return np.copy(action_sample[0].numpy().astype('float32'))\n",
    "    \n",
    "    def learn(self, history_dict):\n",
    "        \"Perform the policy gradient update with its optimizer and policy.\"\n",
    "        obs_tensor = tf.constant(history_dict['observations'], dtype=tf.float32)\n",
    "        returns_tensor = tf.constant(discounted_episode_returns(history_dict['rewards'], gamma=self.gamma), dtype=tf.float32)\n",
    "        actions_tensor = tf.constant(history_dict['actions'], dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_dists = self.policy(obs_tensor)\n",
    "            loss = pg_target(policy_dists,\n",
    "                             returns_tensor,\n",
    "                             actions_tensor)\n",
    "        gradients = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(env, agent, episode_count, max_steps, quiet=False):\n",
    "    done = False\n",
    "    ep_rewards = []\n",
    "    for i in tqdm(range(episode_count)):\n",
    "        reward = 0.0\n",
    "        if not quiet:\n",
    "            print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            if not quiet:\n",
    "                print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "        #agent.learn(history_dict)\n",
    "        history_dict = None\n",
    "        if not quiet:\n",
    "            print('total episode reward: {}'.format(total_reward))\n",
    "        ep_rewards.append(total_reward)\n",
    "    return ep_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp example (not sure if training totally correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845b1960d7a148d0bd3a9a9831080d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_example = gym.make('DynamicSetPacking-silly-v0')\n",
    "ag = MLPMatchAgent(env_example.observation_space.shape, gamma=0.999)\n",
    "ep_rewards = train_loop(env_example, ag, 10, 10, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blood types example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb2720bfa16402e972726b6c9171605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9bb8ef0048ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DynamicSetPacking-gurobitest-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPMatchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mep_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-2bce025b4022>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(env, agent, episode_count, max_steps, quiet)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_onehot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save the onehot version for logprob later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/envs/dynamic_set_packing_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mchosen_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/envs/dynamic_set_packing_env.py\u001b[0m in \u001b[0;36m_perform_match\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/matchers/matcher.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrow_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_sum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquicksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAXIMIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_example = gym.make('DynamicSetPacking-gurobitest-v0')\n",
    "ag = MLPMatchAgent(env_example.observation_space.shape, gamma=0.999)\n",
    "ep_rewards = train_loop(env_example, ag, 300, 50, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare to random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatchAgent:\n",
    "    \"A simple agent for the 0/1 problem that always matches.\"\n",
    "    def __init__(self, match_prob):\n",
    "        self.policy_dist = tfd.OneHotCategorical(probs=[1.0-match_prob, match_prob])\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        action_sample = self.policy_dist.sample()\n",
    "        return action_sample.numpy()\n",
    "    \n",
    "    def learn(self, history_dict):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b775c0b7831943b58b128ca7bc458e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/curry/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/distributions/onehot_categorical.py:172: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5215d084af27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DynamicSetPacking-gurobitest-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomMatchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrandom_ep_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2bce025b4022>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(env, agent, episode_count, max_steps, quiet)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_onehot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save the onehot version for logprob later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/envs/dynamic_set_packing_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mchosen_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/envs/dynamic_set_packing_env.py\u001b[0m in \u001b[0;36m_perform_match\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/dynamic-set-packing-gym/gym_dynamic_set_packing/matchers/matcher.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrow_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_sum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquicksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAXIMIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_example = gym.make('DynamicSetPacking-gurobitest-v0')\n",
    "ag = RandomMatchAgent(0.5)\n",
    "random_ep_rewards = train_loop(env_example, ag, 300, 50, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(random_ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "        agent.learn(history_dict)\n",
    "        print('total episode reward: {}'.format(total_reward))\n",
    "def test_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "        print('total episode reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old pg update func, saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_update(history_dict, policy_model, optim):\n",
    "    \"\"\"Attempt at making a policy gradient update. Seems to be working.\n",
    "    \n",
    "    policy_model should output a TFP dist so we can get the log_prob.\n",
    "    optim is any TensorFlow optimizer (Adam tends to work)\n",
    "    history_dict should contain 'observations', 'rewards', and 'actions' from a rollout.\n",
    "    \"\"\"\n",
    "    # policy_model outputs a TFP distribution\n",
    "    # here, starting to define policy gradient operations. use gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        policy_dists = policy_model(tf.constant(history_dict['observations']))\n",
    "        loss = pg_target(policy_dists, tf.constant(history_dict['rewards']), tf.constant(history_dict['actions']))\n",
    "    gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, policy_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random agent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatchAgent:\n",
    "    \"A simple agent for the 0/1 problem that always matches.\"\n",
    "    def __init__(self, match_prob):\n",
    "        self.policy_dist = tfd.OneHotCategorical(probs=[1.0-match_prob, match_prob])\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        action_sample = self.policy_dist.sample()\n",
    "        return action_sample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_silly_env(RandomMatchAgent(0.3), 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pattern for deep RL algorithms\n",
    "\n",
    "OpenAI has a \"Model\" class that creates two policy networks, the \"act_model\" and the \"train_model\".\n",
    "\n",
    "The \"act_model\" has a \"step\" function (note model.step = model.act_model.step at some point) which takes one step at a time. This is run in the usual way, in a Python loop, to collect one or more trajectories. These trajectories are used as inputs to the train_model to make an update.\n",
    "\n",
    "The act_model and the train_model share weights. In other words there is a core \"policy\" network, shared between both of them. It's just that in the act_model, it gets applied to single states, and there are never any gradients, while in the train_model, it gets applied to batches and there are losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using Keras models, with TFP layer at the end (this is a nice way to represent a policy)\n",
    "\n",
    "One could also have just the Keras model, and manually create TFP distribution objects taking them in. But why not do things this way instead? It makes it really simple.\n",
    "\n",
    "In effect, calling the Module ends up spitting out an object from which you can sample, or get the log_prob.\n",
    "\n",
    "For this categorical case, passing in a batch of 1 will give you a single sample of size 1xactions. Passing in a batch of n gives you a sample of size n by actions. In both cases this is parameterized by the inputs correctly. You can also call sample(n) to get many of these (iid from the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_mlp(action_dim, observation_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_shape=(observation_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(action_dim),\n",
    "        tfp.layers.OneHotCategorical(action_dim)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_policy_mlp(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_input = tf.constant([[1.0,2.0,3.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_result=test(act_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_result.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_input = tf.constant([[1.0,2.0,3.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dist = test(obs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
