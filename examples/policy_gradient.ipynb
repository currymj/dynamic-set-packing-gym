{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'optimizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-daef3da47914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'optimizers'"
     ]
    }
   ],
   "source": [
    "tf.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_dynamic_set_packing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(tf, 'enable_eager_execution'):\n",
    "   tf.enable_eager_execution()\n",
    "# we do want eager execution, because that's the norm for TF2.0\n",
    "# don't forget to test occasionally to see how it works in the alpha, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_example = gym.make('DynamicSetPacking-silly-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.reduce_mean_v1(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(policy_dist, rewards, action_trajectory):\n",
    "    return tf.reduce_mean(policy_dist.log_prob(action_trajectory)*rewards)\n",
    "def not_yet_pg_training(history_dict, policy_model, optim):\n",
    "    # here, starting to define policy gradient operations. use gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        policy_dists = policy_model(tf.constant(history_dict['observations']))\n",
    "        loss = loss_func(policy_dists, tf.constant(history_dict['rewards']), tf.constant(history_dict['actions']))\n",
    "    gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, policy_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "        not_yet_pg_training(history_dict, agent.policy, optimizer)\n",
    "    print('total episode reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "    print('total episode reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp example (note not training yet, just random init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMatchAgent:\n",
    "    def __init__(self, observation_shape):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        # policy network uses TFP layer at the end.\n",
    "        self.policy = tf.keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=observation_shape),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2),\n",
    "            tfp.layers.OneHotCategorical(2)\n",
    "        ])\n",
    "        ## need to compile for non-eager mode?\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        \"Act on a single observation, return an action.\"\n",
    "        observation_as_batch = np.expand_dims(observation, 0)\n",
    "        action_sample = self.policy(observation_as_batch).sample()\n",
    "        return action_sample[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 2. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "total episode reward: 15.0\n"
     ]
    }
   ],
   "source": [
    "train_silly_env(MLPMatchAgent(env_example.observation_space.shape), 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random agent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatchAgent:\n",
    "    \"A simple agent for the 0/1 problem that always matches.\"\n",
    "    def __init__(self, match_prob):\n",
    "        self.policy_dist = tfd.OneHotCategorical(probs=[1.0-match_prob, match_prob])\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        action_sample = self.policy_dist.sample()\n",
    "        return action_sample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 2. 0. 0. 1. 1. 0. 0. 1. 0. 3. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 2. 1. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "total episode reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "test_silly_env(RandomMatchAgent(0.3), 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pattern for deep RL algorithms\n",
    "\n",
    "OpenAI has a \"Model\" class that creates two policy networks, the \"act_model\" and the \"train_model\".\n",
    "\n",
    "The \"act_model\" has a \"step\" function (note model.step = model.act_model.step at some point) which takes one step at a time. This is run in the usual way, in a Python loop, to collect one or more trajectories. These trajectories are used as inputs to the train_model to make an update.\n",
    "\n",
    "The act_model and the train_model share weights. In other words there is a core \"policy\" network, shared between both of them. It's just that in the act_model, it gets applied to single states, and there are never any gradients, while in the train_model, it gets applied to batches and there are losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using Keras models, with TFP layer at the end (this is a nice way to represent a policy)\n",
    "\n",
    "One could also have just the Keras model, and manually create TFP distribution objects taking them in. But why not do things this way instead? It makes it really simple.\n",
    "\n",
    "In effect, calling the Module ends up spitting out an object from which you can sample, or get the log_prob.\n",
    "\n",
    "For this categorical case, passing in a batch of 1 will give you a single sample of size 1xactions. Passing in a batch of n gives you a sample of size n by actions. In both cases this is parameterized by the inputs correctly. You can also call sample(n) to get many of these (iid from the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_mlp(action_dim, observation_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_shape=(observation_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(action_dim),\n",
    "        tfp.layers.OneHotCategorical(action_dim)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_policy_mlp(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_input = tf.constant([[1.0,2.0,3.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_result=test(act_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1040, shape=(1, 2), dtype=float32, numpy=array([[0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_result.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_input = tf.constant([[1.0,2.0,3.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dist = test(obs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
