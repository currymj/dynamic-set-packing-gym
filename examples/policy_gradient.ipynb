{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import gym_dynamic_set_packing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(tf, 'enable_eager_execution'):\n",
    "    tf.enable_eager_execution()\n",
    "# we do want eager execution, because that's the norm for TF2.0\n",
    "# don't forget to test occasionally to see how it works in the alpha, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_example = gym.make('DynamicSetPacking-silly-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_target(policy_dist, rewards, action_trajectory):\n",
    "    \"\"\"The policy gradient target loss (without baseline). Note it has to be negative,\n",
    "    because TF optimizers only want to minimize.\"\"\"\n",
    "    return -tf.reduce_mean(policy_dist.log_prob(action_trajectory)*rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMatchAgent:\n",
    "    def __init__(self, observation_shape):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        # policy network uses TFP layer at the end.\n",
    "        self.policy = tf.keras.Sequential([\n",
    "            layers.Dense(32, activation='relu', input_shape=observation_shape),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2),\n",
    "            tfp.layers.OneHotCategorical(2)\n",
    "        ])\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "        ## need to compile for non-eager mode?\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        \"Act on a single observation, return an action.\"\n",
    "        observation_as_batch = np.expand_dims(observation, 0)\n",
    "        action_sample = self.policy(observation_as_batch).sample()\n",
    "        return action_sample[0].numpy()\n",
    "    \n",
    "    def learn(self, history_dict):\n",
    "        \"Perform the policy gradient update with its optimizer and policy.\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_dists = self.policy(tf.constant(history_dict['observations']))\n",
    "            loss = pg_target(policy_dists, tf.constant(history_dict['rewards']), tf.constant(history_dict['actions']))\n",
    "        gradients = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "        agent.learn(history_dict)\n",
    "    print('total episode reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_silly_env(agent, episode_count, max_steps):\n",
    "    env = gym.make('DynamicSetPacking-silly-v0')\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "    for i in range(episode_count):\n",
    "        print('episode {}'.format(i))\n",
    "        ob = env.reset()\n",
    "        total_reward = 0.0\n",
    "        history_dict = {\n",
    "            'actions': [],\n",
    "            'observations': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        for i in range(max_steps):\n",
    "            history_dict['observations'].append(ob)\n",
    "            \n",
    "            action_onehot = agent.act(ob, reward, done)\n",
    "            history_dict['actions'].append(action_onehot) # save the onehot version for logprob later\n",
    "            action = np.argmax(action_onehot)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            history_dict['rewards'].append(reward)\n",
    "            total_reward += reward\n",
    "            print('action taken: {}, reward: {}, new state: {}'.format(action, reward, env.render()))\n",
    "    print('total episode reward: {}'.format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp example (not sure if training totally correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 2. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 2. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
      "episode 1\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 1. 0. 0. 2. 0. 1. 1. 1. 0. 1. 2. 1.]\n",
      "action taken: 1, reward: 12.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 2. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 1. 0. 3. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 3. 1. 1. 0. 2. 0. 0.]\n",
      "action taken: 1, reward: 9.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 2. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 2. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 1. 1. 0.]\n",
      "episode 2\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 2. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 1. 0. 3. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 3. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 3\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 2. 0. 2. 1. 2.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 1. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 2. 0. 0. 0. 0. 1. 2. 0. 0. 2. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 0. 2. 0. 0. 0. 0. 1. 2. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 2. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 4\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 5\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 6\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 1. 1. 2. 0. 1. 0. 1. 1. 0. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 12.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 7\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 1. 1. 2. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 1. 2. 1. 1. 0. 1. 2. 1. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 1. 1. 2. 1. 0. 2. 1. 2. 2. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 2. 1. 1. 2. 0. 0. 2. 0. 2. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 12.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 3. 0. 1.]\n",
      "episode 8\n",
      "action taken: 1, reward: 16.0, new state: [1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "episode 9\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 1. 2. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 0. 2. 0. 0. 0. 0. 2. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 9.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 10\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 1. 1. 2. 1. 2. 2. 0. 1. 2. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 1. 0. 0. 1. 1. 1. 1. 2. 2. 0. 1. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 2. 0. 0. 1. 0. 1. 1. 2. 1. 0. 1. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 2. 0. 2. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 3. 0.]\n",
      "action taken: 1, reward: 11.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 2. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 11\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 1. 0. 0. 1. 1. 2. 0. 1. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 11.0, new state: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 2. 0. 0. 3. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 2. 0. 0. 3. 0. 0. 0. 0. 0. 1. 2. 0.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 12\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 9.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "episode 13\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 1. 1. 2. 2. 1. 1. 0. 1. 0. 0. 2. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 2. 2. 3. 0. 1. 0. 1. 0. 1. 2. 1. 0.]\n",
      "action taken: 1, reward: 13.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "episode 14\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 15\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 1. 2. 0. 2. 2. 0. 0. 1. 1. 0. 0. 2. 1.]\n",
      "action taken: 1, reward: 13.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 1. 2. 0. 1. 2. 0. 2. 2. 1. 0.]\n",
      "action taken: 1, reward: 12.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "episode 16\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 2. 0. 0. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 17\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      "episode 18\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 2. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 2. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 8.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
      "episode 19\n",
      "action taken: 0, reward: 0.0, new state: [2. 0. 1. 2. 1. 1. 1. 1. 2. 1. 1. 0. 1. 2. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [2. 0. 1. 2. 1. 1. 1. 0. 1. 0. 1. 0. 0. 2. 2. 0.]\n",
      "action taken: 1, reward: 14.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 2. 0. 0. 0. 0. 2. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 3. 0. 0.]\n",
      "episode 20\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 0. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 3. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 0. 1. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 1.]\n",
      "episode 21\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 2. 0. 1. 0. 1. 0. 0. 0. 1. 2. 0. 0. 1.]\n",
      "action taken: 1, reward: 11.0, new state: [0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1.]\n",
      "episode 22\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 23\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "episode 24\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 2. 0. 1. 1. 1. 1.]\n",
      "action taken: 1, reward: 13.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 2. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
      "episode 25\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 0. 0. 2. 1. 0. 2. 2. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 12.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 2. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "episode 26\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "episode 27\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 28\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "episode 29\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 2. 0. 1. 1. 1. 2.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 2. 0. 1. 1. 1. 2.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "episode 30\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 31\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 0.]\n",
      "episode 32\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 33\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 2. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 8.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 34\n",
      "action taken: 0, reward: 0.0, new state: [2. 1. 1. 1. 1. 2. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      "action taken: 1, reward: 15.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 2. 0. 1. 0.]\n",
      "action taken: 1, reward: 8.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 1. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 35\n",
      "action taken: 0, reward: 0.0, new state: [2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 19.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 36\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 2. 1. 0. 0. 1.]\n",
      "episode 37\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 38\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 2. 0. 2. 1. 0. 0. 2. 2. 1. 2. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 1. 3. 1. 0. 1. 3. 2. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 39\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 1. 2. 0.]\n",
      "episode 40\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "episode 41\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 1. 2. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 42\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 43\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 44\n",
      "action taken: 0, reward: 0.0, new state: [0. 2. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "episode 45\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "episode 46\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "episode 47\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 2. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 2. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 9.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "episode 48\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "episode 49\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 50\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "episode 51\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "episode 52\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "episode 53\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "episode 54\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 55\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 2.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 56\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 2. 0.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 57\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 58\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 59\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "episode 60\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "episode 61\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
      "episode 62\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "episode 63\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 64\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 7.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "episode 65\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 2. 0. 1. 1. 0. 1. 0. 0. 1. 0. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 8.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "episode 66\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 67\n",
      "action taken: 1, reward: 16.0, new state: [0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 68\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      "episode 69\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "episode 70\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 71\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 72\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 1. 2. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "episode 74\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "episode 75\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 9.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "episode 76\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 2.]\n",
      "episode 77\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 8.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      "episode 78\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "episode 79\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 80\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 81\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "episode 82\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "episode 83\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 84\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 2. 1. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "episode 85\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 2. 1.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 86\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "episode 87\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "episode 88\n",
      "action taken: 1, reward: 16.0, new state: [1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 89\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "episode 90\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "episode 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 92\n",
      "action taken: 1, reward: 16.0, new state: [1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 9.0, new state: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 93\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 94\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 95\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 96\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "episode 97\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 5.0, new state: [1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 7.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "episode 98\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.]\n",
      "action taken: 1, reward: 6.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "episode 99\n",
      "action taken: 1, reward: 16.0, new state: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 2.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 0.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "action taken: 1, reward: 4.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 1, reward: 3.0, new state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 1, reward: 1.0, new state: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "total episode reward: 34.0\n"
     ]
    }
   ],
   "source": [
    "ag = MLPMatchAgent(env_example.observation_space.shape)\n",
    "train_silly_env(ag, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old pg update func, saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_update(history_dict, policy_model, optim):\n",
    "    \"\"\"Attempt at making a policy gradient update. Seems to be working.\n",
    "    \n",
    "    policy_model should output a TFP dist so we can get the log_prob.\n",
    "    optim is any TensorFlow optimizer (Adam tends to work)\n",
    "    history_dict should contain 'observations', 'rewards', and 'actions' from a rollout.\n",
    "    \"\"\"\n",
    "    # policy_model outputs a TFP distribution\n",
    "    # here, starting to define policy gradient operations. use gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        policy_dists = policy_model(tf.constant(history_dict['observations']))\n",
    "        loss = pg_target(policy_dists, tf.constant(history_dict['rewards']), tf.constant(history_dict['actions']))\n",
    "    gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, policy_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random agent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMatchAgent:\n",
    "    \"A simple agent for the 0/1 problem that always matches.\"\n",
    "    def __init__(self, match_prob):\n",
    "        self.policy_dist = tfd.OneHotCategorical(probs=[1.0-match_prob, match_prob])\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        action_sample = self.policy_dist.sample()\n",
    "        return action_sample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 2. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 2. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 2. 0. 0. 1. 1. 0. 0. 1. 0. 3. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 2. 1. 2. 0. 0. 0.]\n",
      "action taken: 1, reward: 10.0, new state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "action taken: 0, reward: 0.0, new state: [0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "total episode reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "test_silly_env(RandomMatchAgent(0.3), 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General pattern for deep RL algorithms\n",
    "\n",
    "OpenAI has a \"Model\" class that creates two policy networks, the \"act_model\" and the \"train_model\".\n",
    "\n",
    "The \"act_model\" has a \"step\" function (note model.step = model.act_model.step at some point) which takes one step at a time. This is run in the usual way, in a Python loop, to collect one or more trajectories. These trajectories are used as inputs to the train_model to make an update.\n",
    "\n",
    "The act_model and the train_model share weights. In other words there is a core \"policy\" network, shared between both of them. It's just that in the act_model, it gets applied to single states, and there are never any gradients, while in the train_model, it gets applied to batches and there are losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using Keras models, with TFP layer at the end (this is a nice way to represent a policy)\n",
    "\n",
    "One could also have just the Keras model, and manually create TFP distribution objects taking them in. But why not do things this way instead? It makes it really simple.\n",
    "\n",
    "In effect, calling the Module ends up spitting out an object from which you can sample, or get the log_prob.\n",
    "\n",
    "For this categorical case, passing in a batch of 1 will give you a single sample of size 1xactions. Passing in a batch of n gives you a sample of size n by actions. In both cases this is parameterized by the inputs correctly. You can also call sample(n) to get many of these (iid from the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_mlp(action_dim, observation_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_shape=(observation_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(action_dim),\n",
    "        tfp.layers.OneHotCategorical(action_dim)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_policy_mlp(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_input = tf.constant([[1.0,2.0,3.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_result=test(act_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1040, shape=(1, 2), dtype=float32, numpy=array([[0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_result.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_input = tf.constant([[1.0,2.0,3.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0],[1.0,2.0,1.0,4.0,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dist = test(obs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
